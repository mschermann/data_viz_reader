# Fundamentals


  + Theoretical background of data visualization
  
  **Practitioners Guide to Best Practices in Data Visualization**
  
  **Reference**
    Jeffrey D. Camm, Michael J. Fry, Jeffrey Shaffer (2017) A Practitioner’s Guide to Best Practices in Data 
    Visualization.Interfaces 47(6):473-488. https://doi.org/10.1287/inte.2017.0916
  
  These are the best practices of data visualization. Anticipate in advance what kind of questions the viewers will
  ask and then focus your visualization with respect to those questions. Brain processes stimuli from our environment to
  process what is important in 2 ways – unconscious (System 1 represents uncontrolled functions such as facial expressions,
  reactions) and conscious (System 2 – represents controlled function such as solving math problems). Data Visualization
  leverage attributes of System -1 which has can have quick and correct impact in a most efficient manner. The three best
  practices of data visualization are as follows: - 

** 1. Design and layout matter **
      The design and layout should facilitate ease of understanding to convey your message to the viewer.
      
** 2.	Avoid Clutter **
      Keep it simple. To implement this always keep into account the data-ink ratio – the ratio of ink required to
      convey the intended meaning to the total amount of ink used in the table or chart should be as close to 1 as
      possible. That means, avoid ink which do not add any information.
      
** 3.	Use color purposely and effectively **
      Use of color may be prettier and attractive but can be distractive too. Thus, color should be used only if it
      assists in conveying your message. 
      
      The above three principles are illustrated with the help of scenarios and examples which helps to comprehend
      the topic in more meaningful and practical way in the article. It also gives various advantages of using the above 
      principles.And the above best practices could be applied to all the 3 types of analytics: descriptive,
      predictive and prescriptive. 

  
  + Theoretical background of data visualization  
    * https://www.klipfolio.com/blog/intuitive-dashboard-design
    Three rules to follow in order to develop intuitive dashboards:
    
    1. the dashboard should read left to right
    2. group related information together
    3. find relationships between seemingly unrelated areas and display visuals together to show the relationship.
    
    Often a designer can become too concerned with coming up with a visual that is too intricate and overly complicated. A
    dashboard should be appealing but also easy to understand. Following these rules will lead to effective presentation of the
    data. 
    
    Because we read from top to bottom and left to right, a reader's eyes will naturally look in the upper left of a page. The
    content should therefore flow like words in a book. It is important to note that the information at the top of the page
    does not always have to be the most important. Annual data is usually more important to a business but daily or weekly data
    could be used more often for day to day work. This should be kept in mind when designing a dashboard as dashboards are
    often used as a quick convenient way to look up data.
    
    Grouping related data together is an intuitive way to help the flow of the visual. It does not make sense for a user to
    have to search in different areas to find the information they need.
    
    Grouping unrelated data seems contradictory to the second rule, but the important thing is to tell a story not previously
    observed. Data analytics is all about finding stories the data is trying to tell. Once they are discovered, the stories
    need to be presented in an effective manner. Grouping unrelated data together makes it easier to see how they change
    together.
    
    
  + Theoretical background of data visualization
  
    1.  Fundamental Components of Design
          
          Artists use balance, emphasis, movement, pattern, repetition, proportion, rhythm, variety, and 
          unity as the design foundation of any work. If you want to take your data visualization from an 
          everyday dashboard to a compelling data story, incorporate the 9 principles of design from graphic 
          designer Melissa Anderson's article:
                  https://www.idashboards.com/blog/2017/07/26/data-visualization-and-the-9-fundamental-design-principles/
  
          Balance doesn't mean that each side of the visualization needs perfect symmetry, but it is important to have the
          elements of the dashboard/visualiaztion distributed evenly. And it important to remember the non-data elements, such
          as a logo, title, caption, etc., that can affect the balance of the display. 
          
          Another closely related component to balance is variety which could seem counter to balance, but when done
          correctly, variety can help increase the recall of information. However if overdone, too much variety can feel
          cluttered and blur together the images and data in the mind of the viewer.
          
          Arguably the most critical of the components is proportion. Proportion can be subtle but it can go a long way to
          enhancing a viewer's experience and understanding of the data. The danger of proportion though is that it can be
          easy to deceive people subconsciously. Naturally images will have a greater impact on how our brains perceive the
          dashboard or visualization. For example, someone can change the scale of a graph or images to inflate their results
          and even if they write the numbers next to it, the shortcut many people will take is to interpret the data based on
          the image. This is why it is important we take care to accurately reflect proportion in our data visualization and 
          remain critical of how others use proportion in their visualization.
          
          Emphasis was the component that I most related to when reading through the nine principles of design in this
          article. From prior experience with art through photography I understand it is key to be concious of what I am 
          drawing the viewers attention to in my art. When thinking about the art design of data visualization it is also very
          important to remain keen on the main point of your story and how the entire visualization is either drawing the
          viewer to that point of emphasis or how they are being distracted or drawn elsewhere.
          
          
 + Theoretical background of data visualization   
    
    **A Brief History of Data Visualization **
    
    **Reference**

reference-fundamentals-Tableau-Groups

  + Theoretical background of data visualization  
  + Contemporary research results
 contributions
  
## 1. A Brief History of Data Visualization

Michael Friendly,2006,A Brief History of Data Visualization,York University.http://www.datavis.ca/papers/hbook.pdf
   
    The only new thing in the world is the history you don’t know. — Harry S Truman
    
    This paper provides an overview of the intellectual history of data visualization from medieval to modern times,
    describing and illustrating some significant advances along the way.
    
   1. Data Visualization: modern product?
    
       It is common to think of statistical graphics and data visualization as relatively modern developments in statistics. 
       In fact, the graphic representation of quantitative information has deep roots.These roots reach into the histories of 
       the earliest map-making and visual depiction, and later into thematic cartography, statistics and statistical graphics,
       medicine, and other fields.
       
       Developments in technologies (printing, reproduction) mathematical theory and practice, and empirical observation and  
       recording, enabled the wider use of graphics and new advances in form and content.

  2. Milestones Tour
    
       2.1 Pre-17th Century: Early maps and diagrams
          
          Data visualization has comes a long way. Prior to the 17th century, data visualization already exists. Though
          display in other format such as maps, the content are much similar to today's visualization, which mostly presented
          geologic, economic, and medical data.
        
          Here is useful link: http://www.dashboardinsight.com/news/news-articles/the-history-of-data-visualization.aspx
          The earliest seeds of visualization arose in geometric diagrams, in tables of the positions of stars and other
          celestial bodies, and in the making of maps to aid in navigation and exploration. 
       
       2.2 1600-1699: Measurement and theory
       
          Among the most important problems of the 17th century were those concerned with physical measurement— of time,
          distance,and space— for astronomy, surveying, map making, navigation and territorial expansion. This century also
          saw great new growth in theory and the dawnof practical application.
       
       2.3 1700-1799: New graphic forms
       
          With some rudiments of statistical theory, data of interest and importance, and the idea of graphic representation
          at least somewhat established, the 18th century witnessed the expansion of these aspects to new domains and new
          graphic forms. 
          
       2.4 1800-1850: Beginnings of modern graphics  
       
          With the fertilization provided by the previous innovations of design and technique, the first half of the 19th
          century witnessed explosive growth in statistical graphics and thematic mapping, at a rate which would not be
          equalled until modern times.
       
       2.5 1850–1900: The Golden Age of statistical graphics
       
          By the mid-1800s, all the conditions for the rapid growth of visualization had been established— a “perfect storm”
          for data graphics. Official state statistical offices were established throughout Europe, in recognition of the
          growing importance of numerical information for social planning,industrialization, commerce, and transportation. 
          
           2.5.1 Escaping flatland
           2.5.2 Graphical innovations
           2.5.3 Galton’s contributions
           2.5.4 Statistical Atlases
           
       2.6 1900-1950: The modern dark ages
       
          If the late 1800s were the “golden age” of statistical graphics and thematic cartography, the early 1900s can be
          called the “modern dark ages” of visualization. There were few graphical innovations, and, by the mid-1930s, the
          enthusiasm for visualization which characterized the late 1800s had been supplanted by the rise of quantification
          and formal, often statistical, models in the social sciences.
       
       2.7 1950–1975: Re-birth of data visualization
       
          Still under the influence of the formal and numerical zeitgeist from the mid-1930s on, data visualization began to
          rise from dormancy in the mid 1960s. 
          
       2.8 1975–present: High-D, interactive and dynamic data visualization
       
          During the last quarter of the 20th century data visualization has blossomed into a mature, vibrant and multi
          disciplinary research area, as may be seen in this Handbook, and software tools for a wide range of visualization
          methods and data types are available for every desktop computer.
      
      
## 2. Fundamental Components of Design
          
  Artists use balance, emphasis, movement, pattern, repetition, proportion, rhythm, variety, and unity as the design
  foundation of any work. If you want to take your data visualization from an everyday dashboard to a compelling data story,
  incorporate the 9 principles of design from graphic designer Melissa Anderson's article:
    https://www.idashboards.com/blog/2017/07/26/data-visualization-and-the-9-fundamental-design-principles/
  
  Balance doesn't mean that each side of the visualization needs perfect symmetry, but it is important to have the elements of
  the dashboard/visualiaztion distributed evenly. And it important to remember the non-data elements, such as a logo, title,
  caption, etc., that can affect the balance of the display. 
          
  Another closely related component to balance is variety which could seem counter to balance, but when done correctly,
  variety can help increase the recall of information. However if overdone, too much variety can feel cluttered and blur
  together the images and data in the mind of the viewer.
          
  Arguably the most critical of the components is proportion. Proportion can be subtle but it can go a long way to enhancing a
  viewer's experience and understanding of the data. The danger of proportion though is that it can be easy to deceive people
  subconsciously. Naturally images will have a greater impact on how our brains perceive the dashboard or visualization. For
  example, someone can change the scale of a graph or images to inflate their results and even if they write the numbers next
  to it, the shortcut many people will take is to interpret the data based on the image. This is why it is important we take
  care to accurately reflect proportion in our data visualization and remain critical of how others use proportion in their
  visualization.
          
  Emphasis was the component that I most related to when reading through the nine principles of design in this article. From
  prior experience with art through photography I understand it is key to be concious of what I am  drawing the viewers
  attention to in my art. When thinking about the art design of data visualization it is also very important to remain keen on
  the main point of your story and how the entire visualization is either drawing the viewer to that point of emphasis or how
  they are being distracted or drawn elsewhere.
          
          
## 3. Guide to Best Practices in Data Visualization
  
  These are the best practices of data visualization. Anticipate in advance what kind of questions the viewers will ask and
  then focus your visualization with respect to those questions. 
  
  The brain processes stimuli from our environment to process what is important in 2 ways – unconscious (System 1 represents
  uncontrolled functions such as facial expressions, reactions) and conscious (System 2 – represents controlled function
  such as solving math problems). Data Visualization leverage attributes of System -1 which can have a quick and correct
  impact in a most efficient manner. The three best practices of data visualization are as follows: 

** 1. Design and layout matter **
      The design and layout should facilitate ease of understanding to convey your message to the viewer.
** 2.	Avoid Clutter **
      Keep it simple. To implement this always keep into account the data-ink ratio – the ratio of ink required to convey
      the intended meaning to the total amount of ink used in the table or chart should be as close to 1 as possible. That
      means, avoid ink which do not add any information.
** 3.	Use color purposely and effectively **
      Use of color may be prettier and attractive but can be distractive too. Thus, color should be used only if it assists
      in conveying your message. 
      The above three principles are illustrated with the help of scenarios and examples which helps to comprehend the topic
      in more meaningful and practical way in the article. It also gives various advantages of using the above 
      principles.And the above best practices could be applied to all the 3 types of analytics: descriptive, predictive and 
      prescriptive. 

  **Reference**
    Jeffrey D. Camm, Michael J. Fry, Jeffrey Shaffer (2017) A Practitioner’s Guide to Best Practices in Data 
    Visualization.Interfaces 47(6):473-488. https://doi.org/10.1287/inte.2017.0916

## 3.1 Visualization and Graphics Principles to Refocus and Guide You

Jonathon Corum is a graphics designer for The New York Times and he provided a very informative talk to a strictly scientific audience on how to create, design visualizations that explain material originally created for a certain audience, i.e. the scienctific community, but now is to be relayed to a different audience, (in his case, the readership of the Times or maybe the public at large). The talk is filled with examples and break downs of how he has moved from his base content to the final product, all of which are illuminative examples by themselves. There is also great power in the broader themes that he is trying to convey.
First of course is knowing the audience that you are producing the work for, but even in this step, do not lose sight of the ultimate goal of conveying understanding, of explaining a concept. You are searching for a visual idea in your content that can be communicated to your audience. Some of the main highlights to help you make this connection with your audience involve:
Focusing the attention:
  What can be removed? Realize that consistency can help eliminate unecessary distractions. There may be a trade off between losing information but conveying the ultimate meaning more clearly. Label important things rather than relying on a legend, which requires the viewer to hold on to too much information at once.
Involve your audience:
  Give them opportunities to connect their own general knowledge on the topic. Use real world comparisons or examples to help build and relate context. Encourage comparisons and make this easy for the viewer to process and see.
Explain why:
Providing context, adding time sequence details, showing movement, change and mechanism will all guide your audience in connecting the dots and understanding the significance of what you are trying to communicate.

Ref: Jonathon Corum. Presentation to the University of Copenhagen’s Faculty of Health Sciences. http://style.org/ku/

## 4. Survey of Popular Data Visualization Tools

  Due to the rise of big data analytics, there has been an increased need for data visualization tools to help understand the
  data. Besides Tableau, there are several other software tools one can use for data visualization like Sisense, Plotly,
  FusionCharts, Highcharts, Datawrapper, and Qlikview. This article is from forbes and has a brief, clear introduction about
  these 7 powerful software options for data visualization. This could be helpful for future reference because for different
  purposes I may need to use different tools. Each option has its advantages and disadvantages and this article helps
  highlight them.

  **Tableau** is the most popular of the group and has many users. It is simple to use, making it easy to learn and can handle
  large datasets. Tableau can handle big data thanks to integration with database handling applications such as MySQL, Hadoop,
  and Amazon AWS.

  **Qlikview** is the main competitor to Tableau and is also quite popular.  Qlikview is customizable and has a wide range of
  features which can be a double-edged sword. These features take more time to learn and get acquianted with. However, once
  one gets past the learning curve, they have a powerful tool at their disposal.

  The distinctive aspect of **FusionCharts** is that graphics do not have to be created from scratch. Users can start with a
  template and insert their own data from their project.

  **Highcharts** proudly claims to be used by 72% of the 100 biggest companies in the world. It is a simple tool that does not
  require specialized training and quickly generates the desired output. Unlike some tools, Highcharts focuses on cross-
  browser support, allowing for greater access and use.

  **Datawrapper** is making a name for itself in the media industry. It has a simple user interface making it easy to generate
  charts and embed into reports.

  **Plotly** can create more sophisticated visuals thanks to integration with programming languages such as Python and R. The
  danger is creating something more complicated than necessary. The whole point of data visualization is to quickly and
  clearly convey information.

  **Sisense** can bring together multiple sources of data for easier access. It can even work with large datasets. Sisense
  makes it easy to share finished products across departments, ensuring everyone can get the information they need.

  https://www.forbes.com/sites/bernardmarr/2017/07/20/the-7-best-data-visualization-tools-in-2017/#3a12b8ea6c30


## 5. Pick the Right Chart Type!

  Data divusalization is combining the art and science. As for the art, we can say there are no correct answers for doing the
  visualization. There are many ways to present the data. However, how to making sense of facts, numbers and measurement for
  better understanding is still have a logical path to follow. 
  
  To determine which kind of chart is hard for those people new to data visulization. Most people learn it by refering some
  other people's work without understanding the logic behind. So they don't have the theory in their mind to make the
  judgement. Here , I will introduce some guidance to choose the charts. 
  
  When we about to choose the type of chart, we need to answer some questions.
  - How many features would you like to show in a chart?
  - how many data points do you want to display for each variable? 
  - Will you display time serious data or among items or groups. 
  
  After answered this question, you shoul able to get a better imagenation of your ideal graph. The simple guidance for using
  different type of chart is line charts for tracking trends over time, bar charts to compare quantities, scatter plots for
  joint variation of two data items, bubble charts showing joint variation of three data items, and pie charts to compare
  parts of a whole.
  
  Let's review the most commonly used chart types and expalin what circumstance should better use typical chart and the pros
  and conts of each type of chart. Before introduce differnt types of charts, you can use the following website to familiar 
  with different types of charts. 
  [The Data Visualisation Catalogue](https://datavizcatalogue.com/) 
  
  **Type 1 Column Charts.** 
  This should be the most popular chart type. This chart is good to do comparison between different values when specific
  values are important. TBD 

  
  Still have hard time to choose? There are many resources on line can help you do the decision. For example, Dr. Andre Abela
  create a chart selection diagram that is helpful to pick the right chart depends on the data type. The link of website is
  http://extremepresentation.typepad.com/blog/2015/01/announcing-the-slide-chooser.html

  Reference:
  Data Visualization – How to Pick the Right Chart Type? , By Jānis Gulbis
  https://eazybi.com/blog/data_visualization_and_chart_types/

  Data Visualization Best Practices by melindasantos | Sep 19, 2017
  http://paristech.com/blog/data-visualization-best-practices/

  http://paristech.com/blog/data-visualization-best-practices/
  http://extremepresentation.typepad.com/blog/2015/01/announcing-the-slide-chooser.html

  
## 6. Guide for Developing Dashboards

  https://www.klipfolio.com/blog/intuitive-dashboard-design
  Three rules to follow in order to develop intuitive dashboards:
    
    1. the dashboard should read left to right
    2. group related information together
    3. find relationships between seemingly unrelated areas and display visuals together to show the relationship.
    
  Often a designer can become too concerned with coming up with a visual that is too intricate and overly complicated. A
  dashboard should be appealing but also easy to understand. Following these rules will lead to effective presentation of the
  data. 
    
  Because we read from top to bottom and left to right, a reader's eyes will naturally look in the upper left of a page. The
  content should therefore flow like words in a book. It is important to note that the information at the top of the page does
  not always have to be the most important. Annual data is usually more important to a business but daily or weekly data could
  be used more often for day to day work. This should be kept in mind when designing a dashboard as dashboards are often used
  as a quick convenient way to look up data.
    
  Grouping related data together is an intuitive way to help the flow of the visual. It does not make sense for a user to have
  to search in different areas to find the information they need.
    
  Grouping unrelated data seems contradictory to the second rule, but the important thing is to tell a story not previously
  observed. Data analytics is all about finding stories the data is trying to tell. Once they are discovered, the stories need
  to be presented in an effective manner. Grouping unrelated data together makes it easier to see how they change together.
    

## 7. Definions of Date Deception and Graphic Integrity

  Data visualization becomes more and more pupular to communicate and support arguments nowdays. There are lots of great
  resources online to create and design amazing data products, in the same time, there are some poorly-designed misleading 
  deceptive data visualizations.

  So what does **data deception** mean? 
  Data deception, defined by School of Law at the New York University, as “a graphical depiction of information, designed with
  or without an intent to deceive, that may create a belief about the message and/or its components, which varies from the
  actual message.”

  In reality, decades ago, Edward Tufte already introduced the concept of graphical intergrity in his book and presented six
  principles of graphic integrity. Here are the principles from book:

    1. The representation of numbers, as physically measured on the surface of the graphic itself, should be directly
    proportional to the numerical quantities measured.

    2. Clear, detailed, and thorough labeling should be used to defeat graphical distortion and ambiguity. Write out
    explanations of the data on the graphic itself. Label important events in the data.

    3. Show data variation, not design variation.

    4. In time-series displays of money, deﬂated and standardized units of monetary measurement are nearly always better than
    nominal units.

    5. The number of information-carrying (variable) dimensions depicted should not exceed the number of dimensions in the
    data.

    6.Graphics must not quote data out of context.
    
  **Reference** 
 (1) Pandey, A. V., Rall, K., Satterthwaite, M. L., Nov, O., & Bertini, E. (2015). How deceptive are deceptive visualizations?
 An empirical analysis of common distortion techniques. In CHI 2015 - Proceedings of the 33rd Annual CHI Conference on Human
 Factors in Computing Systems: Crossings (Vol. 2015-April, pp. 1469-1478). Association for Computing Machinery. DOI:
 10.1145/2702123.2702608
 (2) Tufte, E. R., and Graves-Morris, P. The visual display of quantitative information, vol. 2. Graphics press Cheshire, CT,
 1983.
  
 **Misleading graphs:**
 
  Misleading graphs or distorted graphs, are graphs created which skews the data, intentionally or unintentionally, resulting
  in a representation of incorrect conclusions.
  
  There are some ways in which distorted graphs can be created:
  1.	Improper scaling of y axis: This is one of the classic misleading graphs. Instead of scale starting from zero or a 
  baseline, y axis is scaled conveniently to highlight the differences among bins.
  2.	Improper labelling of graphs:  Lack of labels make the graph hard to interpret for the reader and lead to wrong 
  conclusions.
  3.	Paired graphs on different scale: It is not a fair comparison if two elements are plotted side-by-side, on a different
  scale and compared. This makes one graph look better than the other, even when it is not.
  4.	Dual axis with different scales: If we are plotting two elements on the same graph with different scales, even if the
  axes are properly labeled, it is assumed that both axes are on the same scale.
  5.	Incomplete data: Short-term graphs are made to manipulate the trend, which will not be seen otherwise. Time-series data
  are cut intentionally to just show a trend within a particular period to create a more favorable visual impression.

  Please find the references below.
  http://hypsypops.com/axes-evil-lie-graphs/
  http://www.statisticshowto.com/misleading-graphs/  
  
  Current research: Deceptive visualizations
  Data visualization is a powerful communication tool to support arguments with numbers in a way that is accessible and
  engaging. More people than ever before are making their own charts and infographics, which is presenting a unique problem.
  Despite the availability of some great charting resources, we are witnessing an influx of poorly-designed misleading or 
  downright deceptive data visualizations.
  Here are useful links: https://medium.com/@Infogram/study-asks-how-deceptive-are-deceptive-visualizations-8ff52fd81239
  https://www.datapine.com/blog/misleading-data-visualization-examples/

## 8. Contemporary Research Results & What's Next
  
  Next Steps for Data Visualization Research

  With the development, studies and new tools applied in data visualization, more people understand it matters. But given its
  youth and interdisciplinary nature, research methods and training in the field of data visualization are still developing.
  So, we asked ourselves: what steps might help accelerate the development of the field? Based on a group brainstorm and
  discussion, this article shares some of the proposals of ongoing discussion and experiment with new approaches:
  
  1. Adapting the Publication and Review Process
    As the article states, "both 'good' and 'bad' reviews could serve as valuable guides", so providing reviewer guidelines
  could be helpful for fledgling practitioners in the field.
     
  2. Promoting Discussion and Accretion
    Discussion of research papers actively occurs at conferences, on social media, and within research groups. Much of this
  discussion is either ephemeral or non-public. So ongoing discussion might explicitly transition to the online forum. 
     
  3. Research Methods Training
    Developing a core curriculum for data visualization research might help both cases, guiding students and instructors
  alike. For example, recognizing that empirical methods were critical to multiple areas of computer science, Stanford CS
  faculty organized a new course on Designing Computer Science Experiments(http://sing.stanford.edu/cs303-sp11/). Also, online
  resources could be reinforced with a catalog of learning resources, ranging from tutorials and self-guided study to online
  courses. Useful examples include Jake Wobbrock’s Practical Statistics for HCI and Pierre Dragicevic’s resources for
  reforming statistical practice.

  Reference: https://medium.com/@uwdata/next-steps-for-data-visualization-research-3ef5e1a5e349
  
## 9. Typography and Data Visualization
  
This article discusses less common applications of typography in data visualization.  While data components such as quantitative or categorical data are commonly represented by visual features like colors, sizes or shapes, utilization of boldface, font variation, and other typographic elements in data visualization are less prevalent.  

Highlighted in the article are preattentive visual attributes; preattentive attributes are those that perceptual psychologists have determined to be easily recognized by the human brain irrespective of how many items are displayed.  Therefore, “preattentive visual attributes are desirable in data visualization as they can demand attention only when a target is present, can be difficult to ignore, and are virtually unaffected by load.”  Examples of preattentive attributes are size/area, hue, and curvature.  

This brings us to the disparateness of the popularity of visual aspects like color and size and typographic aspects such as font variation, capitalization and bold.  The authors present several possible reasons for this, beginning with the preattentiveness of visual attributes like size and hue.  However, some typographic attributes such as line width or size, intensity, or font weight (a combination of the two) are considered preattentive as well.    
Furthermore, these visual attributes are inherently more viscerally powerful, and they are easy to code in a variety of programming languages.  Technology has also perhaps previously limited the use of typographic attributes, for only recently have fine details such as serifs, italics, etc. been made readily visible to the audiences of data visualizations by technological advances.  

Lastly, the authors remark that it is possible the lack of variety of typographic elements used in data visualizations is due to the limited knowledge of computer scientists and other individuals pursuing data visualization in how to apply these elements effectively.  While the first few proposed explanations make sense from personal experience with technology and exposure to data visualizations and design in general, the hypothesis that lack of knowledge of typographic elements in data visualization seems more plausible if it was being applied to a small group of people rather than all of the data visualization design community.  I would say that it is more likely that the use of typographic elements in data visualization is less popular because there are fewer instances in which it can be used appropriately, or a status quo bias—if current visual attributes are received well, the prevailing attitude may be not to fix what is not broken.  
However, the authors also point out that despite the dearth of typographic attributes in data visualization, other spheres like typography, cartography, mathematics, chemistry, and programming “have a rich history with type and font attributes that informs the scope of the parameter space.”

The authors continue by pointing out some tips for using typographic attributes to encode different data types, since certain attributes may be suited to particular purposes.  For example, font weight (size and intensity) is ideal for representing quantitative or ordered data, and font type (shape) is better suited to denote categories in the data.  
Furthermore, as in typography and cartography, use of typographic attributes in data visualization raises concerns of legibility, the ability to understand both individual characters and commonalities that identify a font family, and readability, the ability to read lines and blocks of words.  Often, interactivity of a visualization will not only improve functionality, but also provide a solution to readability issues by providing a means to zoom in on small text. 

There are a few examples of unusual/innovative use of typography for data visualization in the article, not all of which I agree are made more effective by the interesting utilization of typographic attributes, but the “Who Survived the Titanic” visualization’s use of typographic attributes allowed it to not only answer macro-questions very quickly, such as if women and children were actually first to be evacuated across classes, but also to provide answers to micro-questions, like whether or not the Astors survived.  It used common visual elements like color and area to indicate whether or not a person survived and number/proportion of people, as well as typographic aspects like italic and simple text replacement to indicate gender and the passengers’ names.  

![](/images/TypographicTitanic.jpg)

The authors round out the article by addressing the most common criticisms of typography in data visualization, the foremost one being whether or not text should even be considered an element of data visualization, since visualization connotes preattentive visual encoding of information, and text or sequential information necessitates more investment of attention to understand.  Another criticism is that textual representations are not as visually appealing even when used effectively.  However, the authors counter that “this criticism indicates both the strength and weakness of type,” that while text may not be suited for adding style or drama to a visualization, it can be particularly powerful in situations where a finer level of detail is needed, without sacrificing representation of higher level patterns.  Lastly, a label length problem is common when using text in visualizations; differing lengths of names or labels may skew perception so that longer labels seem more important than shorter labels.  This problem was encountered in the Titanic visualization with the varying lengths representations of passengers’ names, and was corrected by only including a given name and a surname, the length of which could only vary so much.    

All in all, this article has an interesting take on a somewhat less fashionable tool and puts forth the idea that text and typographic attributes can convey additional important information in data visualizations when used innovatively and correctly.    


Reference: Banissi, Ebad, & Brath, Richard. (2016).  Using Typography to Expand the Design Space of Data Visualization. She Ji: The Journal of Design, Economics and Innovation, 2(1), pp 59-87. https://www.sciencedirect.com/science/article/pii/S2405872616300107.  

[@data-insights]


##This article explains 9 design principles which can be used for vizulation. These 9 design principles are:
https://www.idashboards.com/blog/2017/07/26/data-visualization-and-the-9-fundamental-design-principles/

1.	Balance: A design is said to be balanced if key visual elements such as color, shape, texture, and negative space      are uniformly distributed.

2.	Emphasis: Draw viewers attention towards important data by using key visual elements.

3.	Movement: Ideally movement should mimic the way people usually read, starting at the top of the page, moving    across it, and then down.  Movement can also be created by using complimentary colors to pull the user's attention  across the page.

4.	Pattern: Patterns are ideal for displaying similar sets of information, or for sets of data that equal in value.      Disrupting the pattern can also be effective in drawing viewers attention; it naturally draws curiosity.

5.	Repetition: Relationships between sets of data can be communicated by repeating chart types, shapes, or colors.

6.	Proportion: If a person is portrayed next to a house, the house is going look bigger. In data visualization,     proportion can indicate the importance of data sets, along with the actual relationship between numbers. 

7.	Rhythm: A design has proper rhythm when the design elements create movement that is pleasing to the eye. If the      design is not able to do so,  rearranging visual elements may help.

8.	Variety: Variety in color, shape, and chart-type draws and keeps users engaged with data. Including more variety     can increase information retention by the viewer. But when there is too much variety, important details can be       overlooked.

9.	Unity: Unity across design will happen naturally if  all other design principles are implemented. 


**Using Data Visualization to Find Insights in Data**


https://www.educba.com/data-mining-vs-data-visualization/

This article gives me a clear understanding of data mining and data visualization.

https://www.educba.com/data-mining-vs-data-visualization/

This article gives me a clear understanding of data mining and data visualization.

In Data Mining, there are different processes involve carrying out the data mining process such as data extraction, data management, data transformations, data pre-processing, etc.
In Data Visualization, the primary goal is to convey the information efficiently and clearly without any deviations or complexities in the form of statistical graphs, information graphs, and plots. 
Also, the author listed the top 7 comparisons between data mining and data visualization, and 12 key differences between data mining and data visualization. After reading the article, you will have a very clear understanding of what are data mining and data visualization and the characters for those two techniques. 

In Data Mining, there are different processes involve carrying out the data mining process such as data extraction, data management, data transformations, data pre-processing, etc.
In Data Visualization, the primary goal is to convey the information efficiently and clearly without any deviations or complexities in the form of statistical graphs, information graphs, and plots. 
Also, the author listed the top 7 comparisons between data mining and data visualization, and 12 key differences between data mining and data visualization. After reading the article, you will have a very clear understanding of what are data mining and data visualization and the characters for those two techniques.

 **Reference Link**
 http://datajournalismhandbook.org/1.0/en/understanding_data_7.html 
 
 This article is extracted from a book known as Data Journalism Handbook and this is one of the chapters of the book. The author starts the article by introducing a very simple idea that loading any dataset into a spreadsheet can also be a form of visualization as an invisible data becomes visible in a picture form into a table. Hence the focus should not be whether we need data visualization or not but should be on which form of data visualization is best in which situation.
 
 The author then proceeds by stating that data visualization will not always unleash a readymade story on its own. Sometimes the insights are known before the visualization and sometimes an insight can be completely new. The author has given a process for finding insights in the following way:
 
 Visualize Data-> Analyze -> Document Insights -> Transform Datasets ->Visualize Data
 
 Each stage is explained in-depth further. Data Visualization can be done in many ways such as tables which are great for one dimensional data however they are bad for multi-dimensional data. Then he goes further to explain the situation where each type of visualization such as bar charts, maps, scatterplots, graphs, etc. are used. This gives a thorough understanding of when to use which type of visualization. Once we visualize the data we need to ask the following questions:
 
 1 What can I see in this image? Is it what I expected?
 2	Are there any interesting patterns?
 3	What does this mean in the context of the data?
 
 The basic question answer format gives an idea to the viewers about what kind of perspectives can we look at the data. Sometimes we discover something and sometimes we don’t. But the author mentions that we always learn something from the visualization. Once we document the data insights based on the above question we need to have the following points into consideration:
 
 1 Why have I created this chart?
 2	What have I done to the data to create it?
 3	What does this chart tell me?
 
 The above question answer format compels the viewers to think deeper about what exactly we are trying to find. Because many times the viewers are simply too overwhelmed with the size of data that they lose the basic idea. Hence this kind of approach help to stay focused. The author then mentions that based on the above insights we might have some idea about some interesting patterns. Since we already have an idea we might want to see it in more detail and hence we transform data in more details such as Zooming, Filtering, Outlier Removal. The author then explains how transformed data can help us to see a more detailed view of our insights. 
 
 Further the author gives a detailed explanation of which data visualization tool to use based on the situation. The entire process given above is explained in depth with the help of examples. The technical approach listed above is practical and can be implemented easily on our data visualization projects. I liked the author’s approach because he has cleverly integrated the step-by-step process of finding insights with the technical way of handling datasets using tools such as Tableau, Python, etc. And the process can be repeated many times till we find the insights we are looking for.
 
 
 
 Building advanced analytics application with TabPy
 
 https://www.tableau.com/about/blog/2017/1/building-advanced-analytics-applications-tabpy-64916
 
 @misc{great_viz,
   author = {{Bora Beran}},
   year = {2017},
   title = {Building advanced analytics applications with TabPy},
   howpublished = {\url{https://www.tableau.com/about/blog/2017/1/building-advanced-analytics-applications-tabpy-64916}},
   note = {Accessed: 2018-04-28}
 }
 
 
 Imagine a scenario where we can just enter some x values in a dashboard form, and the visualization would predict the y variable!!! 
 Here is a link that shows how to integrate and visualize data from Python in Tableau. This is especially relevant to all data science students, as this is one of the tools used for visualizing advanced analytics. 
 The author here has given an example using data from Seattle's police department's 911 calls and he tries to identify criminal hotspots in the area.  The author uses machine learning (spatial clustering) and creates a great interactive visualization, where you can click on the type of criminal activity and the graph will show various clusters. 
 There are other examples and use cases that may be downloaded, and the scripts are also given by the author for anyone who is interested in trying it out. 
 
    + Theoretical background of data visualization  
   + Contemporary research results
 
 Some best practices for visualization:
 
   http://www.dataplusscience.com/files/visual-analysis-guidebook.pdf
   
 Here is free pdf to some best practices in visual analysis. It talks about the right charts to be used for various kinds of analysis. It is very relevant for data science students as we would be interested in presenting our analysis using simple and effective visualizations that tell the complete story. 
 
 Some of key areas for which the author highlights some best practices are for visualizing trends over time, comparison and ranking, correlation, distribution, geographical data etc. 
 
 The author gives examples on how simple graphs can also become more effective by just adding a few more elements or some simple adjustments. 
 
 I feel this is a great starting point to create effective charts and we may use these principles also when we start doing advanced analytics.
 
 contributions
 
 ###Interactive Data Visualization
 
 Interactive or Dynamic data visualization delivers today’s complex sea of data in a graphically compelling and an easy-to-understand way. It enables direct actions on a plot to change elements and link between multiple plots. It enables users to accomplish traditional data exploration tasks by making charts interactive.
 
 ####Benefits of Interactive Data Visualization Software:
 
 1. Absorb information in constructive ways: With the volume and velocity of data created everyday, dynamic data viz enables enhanced process optimization, insight discovery and decision making.
 2. Visualize relationships and patterns: Helps in  better understanding of correlations among operational data and business performance.
 3. Identify and act on emerging trends faster: Helps decision makers to grasp shifts in behaviors and trends across multiple data sets much more quickly.
 4. Manipulate and interact directly with data: Enables users to engage data more frequently.
 5. Foster a new business language : Ability to tell a story through data that instantly relates the performance of a business and its assets.
 
 [@benefits_interactive_viz]
 
 There are multiple ways by which interactive data visualizations can be developed:
 1. D3.js  2.Tableau  3.R shiny
 
 ####D3.js:
 
 D3.js (or just D3 for Data-Driven Documents) is a JavaScript library for producing dynamic, interactive data visualizations in web browsers(From Wikipedia). It is highly functional, meaning you can reuse the code and add functions relevant to your project. Embedded within an HTML webpage, the JavaScript D3.js library uses pre-built JavaScript functions to select elements, create SVG objects, style them, or add transitions, dynamic effects or tooltips to them.
 
 Some of the key advantages are: It is dynamic, free and open source and very flexible with all web technologies, the abiity to handle big data and the functional style allows to reuse the codes.
 
 [@d3_interactive_viz]
 
 ####Tableau:
 
 Tableau is business intelligence (BI) and analytics platform created for the purposes of helping people see, understand, and make decisions with data. It is the industry leader in interactive data visualization tools, offering a broad range of maps, charts, graphs, and more graphical data presentations. It is a painless option when cost is not a concern and you do not need advanced and complex analysis.The application is very handy for quickly visualizing trends in data, connecting to a variety of data sources, and mapping cities/regions and their associated data.
 
 The key advantages are: It provides non technical user the ability to build complex reports and dashboard with zero coding skills. Using drag-n-drop functionalities of Tableau, user can create a very interactive visuals within minutes. It can handle millions of rows of data with ease and users can make live to connections to different data sources like SQL etc.
 
 [@tableau_interactive_viz]
 
 Now that we have all learnt the basics of working with Tableau, we may think of following groups or joining communities to explore tableau further.

The benefits:

-	It will help us enhance our learning
-	Get answers for most of your doubts In tableau
-	Post new questions and crowd source answers
-	Attend events, seminars and join conferences conducted locally/ globally
-	Give back to the community once you become an expert in that field

Some useful communities for Tableau users:

Tableau Community - [@Tableau_Community]

Blogs : Here is a list of the top 10 blogs that Tableau itself suggests following:

[@Top_10_Blogs]

1.	Storytelling with Data
2.	Information is Beautiful
3.	Flowing Data
4.	Visualising Data
5.	Junk Charts
6.	The Pudding
7.	The Atlas
8.	Graphic Detail
9.	US Census and FEMA
10.	Tableau Blog

Tableau Social Media Groups: Some of the biggest and the most active groups

Tableau Enthusiasts: Linkedin Group (19K members)

Tableau Software Fans & Friends: LinkedIn Group (45kK members)

[@LinkedIn_Groups] 

 
 ####R Shiny :
 
 R Shiny enables us to produce interactive data visualizations with a minimum knowledge of HTML, CSS, or Java using a simple web application framework that runs under the R statistical platform. Standalone apps can be hosted on a webpage or embedded in R Markdown documents and dashboards can be built using R shiny. It combines the computational power of R with the interactivity of the modern web.
 
 The main advantages of using R Shiny are : Its flexibility of pulling in whatever package in R that you want to solve your problem, reaping the benefits of an open source ecosystem for R and Javascript visualization libraries, thereby allowing to create highly custom applications and enabling timely, high quality interactive data experience without (or with much less) web development and without the limitations or cost of proprietary BI tools.
 
 [@shiny_interactive_viz]


#Avoiding Common Mistakes with Time Series
https://www.svds.com/avoiding-common-mistakes-with-time-series/

This article explains how time series data visualization can sometimes be deceptive.  It first takes an example of two random time series data and plots them on a graph which gives an impression that the two are strongly correlated. But if we do some statistical testing the two do not show any relationship, this is an example of "correlation does not necessary mean causation".  In another set of examples author has taken trending two random time series data and shown how even statistical tests can give a wrong interpretation. The article then explains using visualization how a general trended time series can be different than a more controlled and measured trending time series.
master


## 10. Corporate Scorecards and Data Visualization
  - (reference: http://www.boostlabs.com/corporate-scorecards-data-visualization/)

Corporate transparency, flat organizations, open book policies, etc. are terms executives and entrepreneurs learn about all the time. As the corporate world shifts towards a more open culture, the demand for open data and insights have increased dramatically. This shift has helped the overall corporate strategic planning and management process–easing the alignment of business activities towards a series of goals. Being transparent top down aligns the culture to sail towards the same North Star. 

The growth of corporate transparency is not only important internally, but externally as well. Corporate certifications like B Corporations certifications (B Corp), require companies to provide a transparent view on their social conscious efforts to the general public. Achieving the certification is one step of the process; the true goal is to show the world how and why the certification is truly deserved.

How does data visualization play a part? Data Visualization helps reveal insights and patterns that aren’t immediately visible in the raw data. 
Here’s the process on how to get it done:

**Step 1: Perform Data Discovery and Determine The Story**
Before this step it is easy to underestimate the effort level it takes to pull the best insights from the data. Data manipulation products like Tableau, Domo, Pentaho, IBM’s Many Eyes, and R, among others, make insight extraction that much easier to gain understanding of data using a visual medium.

The key is to start with a simple portion of your data and to start pulling basic insights to visualize and correlate with each other. This process leads towards a compound series of questions, which helps provide an overall vision to the end product. We see the effect during our discovery process, which leads to unforeseen avenues for data intelligence.

**Step 2: Data Infrastructure Setup**
Data infrastructures can be simple or complex depending what the end goal is. Many clients prefer to go the route of complete data integration in order to centralize their data repositories. Technologies such as Hadoop have helped by unifying disparate data sources, but other options such as data cloud environments can help produce API’s for future product deployments. Why is this important? Accessibility of data is an important foundation not only within the context of dashboards, but also the possibility of branching out to other products.

**Step 3: Product Design & Development**
Wireframing, prototyping, and application development are the main engines to transform an idea into a final product. Products can range from static presentations/reports to full interactive applications. Mobile, tablet, TV, and workstation platforms can all be mediums to help deliver the final product. The secret to a great end product is how well the data story is conceptualized. If the story is weak then the end product will also suffer. 

**Step 4: QA & Product Release**
The best part of any project is to get it finalized and released for all to see. All data gets verified for accuracy, functionality testing (if applicable), application flow (if applicable), design testing, and remaining items are all completed. The end result is an engaging visual product for all intended audiences to see and use.

# Data Visualization Tools

Lisa Rost's article "What I learned recreating one chart using 24 tools" describes lessons learned from recreating one chart using many different data visualization tools.  The author used apps Excel, Plotly, Easycharts, Google Sheets, Lyra, Highcharts, Tableau, Polestar, Quadrigram, Illustrator, RAW, and NodeBox, as well as charting libraries ggvis, Bokeh, Highcharts, ggplot2, Processing, NVD3, Seaborn, Vega, D3, matplotlib, Vega-Lite, and R.  She links her github page on the project which details the dataset she used, containing the health expectancy in years as well as GDP per capita and population for about 200 countries in the year 2015, as well has her process and results of visualizing the data using each tool.  However, in the article, she focuses on the main takeaways from the exercise, which was especially interesting in the context of our class discussion on different types of tools and their respective strengths.  She also provides her own graphics to help illustrate her lessons learned.  

Rost’s first takeaway: **“There Are No Perfect Tools, Just Good Tools for People with Certain Goals”**

Since data visualization is necessary in many spheres, from science to journalism, data visualization projects will often have quite disparate objectives, and the people working on them will have different requirements.  And as the author aptly points out, it is impossible for one tool to satisfy the needs of every data visualizer; so there will necessarily be tools better suited to specific situations.  

For example, does the user need a tool for exploratory visualization of the data, or does the user seek to create graphs and charts to show the public or a specific audience something?  

![](/images/analysis_spectrum.png)

The author also notes that the flexibility of a tool is a sticking point as well—if you need to change your data while developing a data visualization, certain apps like Illustrator will not be ideal because changing the data even slightly requires you to build the graph again from scratch.  Another thing to think about is the type of chart you are trying to create—is a basic, canned bar or line graph all you need (in which case something like Excel will do the trick), or does your project necessitate a more innovative or custom chart (like something possible in D3.js)?  Interactivity is another big question—only certain tools will make this possible.

![](/images/interactivity.png)

Rost’s next takeaway: **“There Are No Perfect Tools, Just Good Tools for People with Certain Mindsets”**

This section of the article is all about the difference in people’s preferences and opinions; from the people who build the tools to the users, everyone thinks differently.  Therefore, certain tools will be inherently more intuitive to use for different people.  


Rost’s third lesson: **“ We Still Live in an ‘Apps Are for the Easy Stuff, Code Is for the Good Stuff’ World”**
Basically, writing code can be scary for anyone without a coding background, but it provides more flexibility, and, as mentioned in class, code is perfectly reproducible.  On the other hand, apps are much more user-friendly for the less computer science-savvy.  

![](/images/apps_vs_code.png)

Rost’s final lesson: **“‘Every Tool Forces You Down a Path’”**

Rost quotes her former NPR Visuals teammate for the final lesson header, pointing out that tools themselves influence the development of a data visualization with their respective features, strengths, and limitations.  

![](/images/tools_force_paths.png)

reference: [@different_tools]




 Reference--Visual-Lies
** Visual Lies: Usability in Deceptive Data Visualizations **
 Reference - [@visual-lies]


The article focuses on a few methods that data visualizers utilize to mislead users about research findings. For each method, the author has highlighted the signifiers that are manipulated to promote an unrealistic understanding of the visualized data. The author has concentrated on examples of three areas to create deceptive data visualization: size, segmentation, and graph type.

** Size **
Size signifies quantity, volume or degree of variables within a data. In first figure the y-axis from the graph to the right is cut when transcribed onto the graph on the left. Here both the graphs show the same data but the one on the left represents the data in a misleading fashion because of the way the axis is cut, and the result is that interest rates have increased drastically from 2008 to 2012 – a misinterpretation that is avoided in the graph on the right. 

Figure 1:
```{r echo=FALSE, out.width='100%'}
knitr::include_graphics('./Size1.jpg')
```

Quantity is the measure of size. When depicting points on a scatter plot, the author suggest that it is helpful to manipulate the size the points to represent differing values of a variable that is not represented on the x and y axes. Following graph shows quantity as a two completely different measure. One chart uses quantity as Area and other uses it as radius. The result is that the differences in quantity between points on such a scatter plot would appear more dramatic than they should be.

Figure 2:
```{r echo=FALSE, out.width='100%'}
knitr::include_graphics('./Quantity1.jpg')
```

** Segmentation **
Figure shows an example of this with a deceptive instance of binning given in the legend on the left.
Segmentation can be used to show category, parts, domains or ranges within a chart. The author states that correct use of segmentation can enable users to enhance understanding and if used incorrectly can be deceptive. It is shown here binning is different in both and since in the left figure binning is not done appropriately it is difficult to come up with actual values of the data.

Figure 3:
```{r echo=FALSE, out.width='100%'}
knitr::include_graphics('./Segmentation1.jpg')
```

** Graph **
Two graphs that are most often misrepresented are pie-charts and maps. The author explains that in the following figure Pie charts can’t be compared accurately to one another. When striving for an accurate portrayal of values, they should be avoided. The author further states that it would be difficult to understand the pie-charts had the numbers weren’t given.

Figure 4:
```{r echo=FALSE, out.width='100%'}
knitr::include_graphics('./PieCharts.jpg')
```

The author then states that when showing spatial data analysis always show population density when visualizing values that are person-dependent. On a heat map where color signifies quantity, The author suggests that a user will be drawn to the colors that a legend indicates are most extreme. 
In following figure, areas that are darkest are simply the most population-dense regions of the United States. Without accounting for population density, the newly created map may look the same as hundreds of maps bearing a striking resemblance to the figure, which are falsely considered informative and are regularly shared across social media sites.

Figure 5:
```{r echo=FALSE, out.width='100%'}
knitr::include_graphics('./Maps1.jpg')
```

The above pointers are very helpful when creating a deceptive version of a data product. However, as data visualizers we carefully need to draw the line between creating misleading graphs that tells a different story and deceptive version which is meant for exaggeration. The above can be applied in our projects and can also be used to enhance our understanding of great data visualization product.


http://paristech.com/blog/data-visualization-best-practices/
http://extremepresentation.typepad.com/blog/2015/01/announcing-the-slide-chooser.html

+ Tufte's Design Principles 
  * Reference: "The visual display of Quantitative information", Edward R.Tufte
  * Note: all figure without another source mark, it comes from the book above.
  A graph should be impressive and can obtain audience's attention. How can we achieve this? We must follow some principles for statistical graphics:
  **Principle 1: Maximizing the data-ink ratio, within reason.**
  Data-ink is the non-erasable core of a graphic, the non-redundant ink arranged in response to variation in the numbers represented.

   ** Data-ink ratio = data-ink/total ink used to print the graphic = proportion of graphic’s ink devoted to the non-redundant display of data-information**
	
	 This basic principle follows by two principles:
	 ** 1. Erase non-data-ink, within reason. **
	 ** 2. Erase redundant data-ink, within reason.**
	 Examples:
	
1. 
	 
figure1:
	 
2. 
figure2
figure3
source: https://medium.com/@plotlygraphs/maximizing-the-data-ink-ratio-in-dashboards-and-slide-deck-7887f7c1fab
	 
   **3. always revise and edit**
	 Example:
	 figure 4
	 
	 ** Principle 2: Multifuncitioning elements**
	 “Mobilize every graphical element, perhaps several >mes over, to show the data”
Vol1, pg 139
   In other words, try to make all present graphical elements data encoding elements.
	 Example:
	 figure 5
	 
	 ** Principle 3: maximize data density**
	 Maximize data density and the size of the data matrix within reason.
	 Data Density = # entries in data matrix /area of data graphic
	 
	 1. Maximize data density and the size of the data matrix within reason
	 Example:
	 figure 6
	 
	 2. Reduce the size of the graphic = the Shrink Principle.
	 Example:
	 figure 7
	 
	 Composition of design principles
1. Escape flatland – small multiples, parallel sequencing
Data is multivariate.Doesn’t necessarily mean 3D projection
How can we enhance mulitvariate data on inherently 2D surfaces?
figure 8 small multiples
figure 9 parallel sequencing

2. Macro/Micro-Provide the user with both views (overview and detail)
figure 10 Envisioning Information

3.Utilize Layering & Separation
Supported by Gestalt laws
•  Grouping with colors
•  Using Color to separate
•  1+1 = 3 (clutter)
figure 11 Utilize Layering & Separation

4.Utilize narratives of space and time
Tell a story of position and chronology through visual elements.
figure 12 
https://guns.periscopic.com/?year=2013



	 


	 
